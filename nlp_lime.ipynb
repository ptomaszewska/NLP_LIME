{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/lib/python3.7/site-packages/')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data(data_dir):\n",
    "    tensors_path, labels_path = '', ''\n",
    "    print(data_dir)\n",
    "    for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "        print (dirpath)\n",
    "        for f in filenames:\n",
    "            if f == 'metadata.tsv':\n",
    "                labels_path = os.path.join(dirpath, f)\n",
    "            elif f == 'tensors.tsv':\n",
    "                tensors_path = os.path.join(dirpath, f)\n",
    "    if not (tensors_path and labels_path):\n",
    "        raise RuntimeError(f'Could not find required files!')\n",
    "    return tensors_path, labels_path\n",
    "    \n",
    "\n",
    "def load_part_data(data_dir):\n",
    "    X_path, y_path = find_data(data_dir)\n",
    "    print(X_path)\n",
    "    y = pd.read_csv(y_path, sep=':', encoding='utf-8', header=None).rename(columns={0:'job', 1:'name'})\n",
    "    X = pd.read_csv(X_path, sep='\\t', encoding='utf-8', header=None)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_data(data_dir, validation=True):\n",
    "    X_train, y_train = load_part_data(os.path.join(data_dir, 'train'))\n",
    "    X_valid, y_valid = load_part_data(os.path.join(data_dir, 'validation'))\n",
    "    X_test, y_test = load_part_data(os.path.join(data_dir, 'test'))\n",
    "    if not validation: \n",
    "        X_train = X_train.append(X_valid, ignore_index=True)\n",
    "        y_train = y_train.append(y_valid, ignore_index=True)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\train\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\train\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\train\\tensors.tsv\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\validation\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\validation\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\validation\\tensors.tsv\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\test\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\test\n",
      "C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET\\test\\tensors.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5033, 5033, 484, 484, 299, 299)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_valid, y_valid = load_data(r'C:\\Users\\Paulina\\Downloads\\mean_data.tar\\mean_data\\BERT\\ONET', validation=True)\n",
    "len(X_train), len(y_train), len(X_test), len(y_test), len(X_valid), len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train['job']\n",
    "y_valid=y_valid['job']\n",
    "y_test=y_test['job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5033 samples, validate on 299 samples\n",
      "Epoch 1/30\n",
      "5033/5033 [==============================] - 1s 222us/sample - loss: 1.0007 - accuracy: 0.7473 - val_loss: 1.0710 - val_accuracy: 0.7291\n",
      "Epoch 2/30\n",
      "5033/5033 [==============================] - 0s 83us/sample - loss: 0.8386 - accuracy: 0.7749 - val_loss: 1.1004 - val_accuracy: 0.7291\n",
      "Epoch 3/30\n",
      "5033/5033 [==============================] - 0s 86us/sample - loss: 0.7392 - accuracy: 0.7963 - val_loss: 1.2491 - val_accuracy: 0.7291\n",
      "Epoch 4/30\n",
      "5033/5033 [==============================] - 0s 88us/sample - loss: 0.6790 - accuracy: 0.8067 - val_loss: 1.0589 - val_accuracy: 0.7492\n",
      "Epoch 5/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.6326 - accuracy: 0.8254 - val_loss: 1.0242 - val_accuracy: 0.7492\n",
      "Epoch 6/30\n",
      "5033/5033 [==============================] - 0s 89us/sample - loss: 0.5842 - accuracy: 0.8281 - val_loss: 1.1001 - val_accuracy: 0.7458\n",
      "Epoch 7/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.5370 - accuracy: 0.8385 - val_loss: 1.0318 - val_accuracy: 0.7324\n",
      "Epoch 8/30\n",
      "5033/5033 [==============================] - 0s 88us/sample - loss: 0.4897 - accuracy: 0.8508 - val_loss: 1.3336 - val_accuracy: 0.7458\n",
      "Epoch 9/30\n",
      "5033/5033 [==============================] - 0s 89us/sample - loss: 0.4524 - accuracy: 0.8601 - val_loss: 1.2126 - val_accuracy: 0.6622\n",
      "Epoch 10/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.4207 - accuracy: 0.8681 - val_loss: 1.5506 - val_accuracy: 0.7425\n",
      "Epoch 11/30\n",
      "5033/5033 [==============================] - 0s 86us/sample - loss: 0.3969 - accuracy: 0.8762 - val_loss: 1.3775 - val_accuracy: 0.7358\n",
      "Epoch 12/30\n",
      "5033/5033 [==============================] - 0s 90us/sample - loss: 0.3601 - accuracy: 0.8856 - val_loss: 1.4741 - val_accuracy: 0.7525\n",
      "Epoch 13/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.3235 - accuracy: 0.8939 - val_loss: 1.4343 - val_accuracy: 0.5987\n",
      "Epoch 14/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.2992 - accuracy: 0.9032 - val_loss: 1.3149 - val_accuracy: 0.7057\n",
      "Epoch 15/30\n",
      "5033/5033 [==============================] - 0s 88us/sample - loss: 0.2874 - accuracy: 0.9076 - val_loss: 1.3617 - val_accuracy: 0.6890\n",
      "Epoch 16/30\n",
      "5033/5033 [==============================] - 0s 86us/sample - loss: 0.2547 - accuracy: 0.9146 - val_loss: 1.7620 - val_accuracy: 0.7592\n",
      "Epoch 17/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.2311 - accuracy: 0.9257 - val_loss: 1.5422 - val_accuracy: 0.7090\n",
      "Epoch 18/30\n",
      "5033/5033 [==============================] - 0s 85us/sample - loss: 0.2271 - accuracy: 0.9251 - val_loss: 1.6220 - val_accuracy: 0.7425\n",
      "Epoch 19/30\n",
      "5033/5033 [==============================] - 0s 88us/sample - loss: 0.1919 - accuracy: 0.9352 - val_loss: 1.7903 - val_accuracy: 0.7224\n",
      "Epoch 20/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.1864 - accuracy: 0.9356 - val_loss: 1.6637 - val_accuracy: 0.6622\n",
      "Epoch 21/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.1634 - accuracy: 0.9424 - val_loss: 1.9226 - val_accuracy: 0.6823\n",
      "Epoch 22/30\n",
      "5033/5033 [==============================] - 0s 88us/sample - loss: 0.1546 - accuracy: 0.9450 - val_loss: 1.9542 - val_accuracy: 0.6756\n",
      "Epoch 23/30\n",
      "5033/5033 [==============================] - 0s 86us/sample - loss: 0.1503 - accuracy: 0.9547 - val_loss: 2.0696 - val_accuracy: 0.6488\n",
      "Epoch 24/30\n",
      "5033/5033 [==============================] - 0s 86us/sample - loss: 0.1353 - accuracy: 0.9585 - val_loss: 2.1049 - val_accuracy: 0.7224\n",
      "Epoch 25/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.1373 - accuracy: 0.9605 - val_loss: 2.0344 - val_accuracy: 0.6957\n",
      "Epoch 26/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.1090 - accuracy: 0.9636 - val_loss: 2.4944 - val_accuracy: 0.7258\n",
      "Epoch 27/30\n",
      "5033/5033 [==============================] - 0s 86us/sample - loss: 0.1062 - accuracy: 0.9648 - val_loss: 2.6234 - val_accuracy: 0.6756\n",
      "Epoch 28/30\n",
      "5033/5033 [==============================] - 0s 87us/sample - loss: 0.1081 - accuracy: 0.9644 - val_loss: 2.3680 - val_accuracy: 0.6890\n",
      "Epoch 29/30\n",
      "5033/5033 [==============================] - 0s 88us/sample - loss: 0.0834 - accuracy: 0.9736 - val_loss: 3.0668 - val_accuracy: 0.7324\n",
      "Epoch 30/30\n",
      "5033/5033 [==============================] - 0s 85us/sample - loss: 0.0838 - accuracy: 0.9722 - val_loss: 3.1186 - val_accuracy: 0.5652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e3a8267348>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = y_train.unique()\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(X_train.columns),)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(len(classes), activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "le = LabelEncoder()\n",
    "\n",
    "model.fit(\n",
    "    X_train.to_numpy(),\n",
    "    to_categorical(le.fit_transform(y_train)),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid.to_numpy(), to_categorical(le.transform(y_valid))),\n",
    "    #validation_freq=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 9, 7, 9, 1, 7, 9, 9, 7, 7, 7, 2, 1, 7, 7, 7, 9, 7, 7, 7, 7,\n",
       "       7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       9, 7, 7, 7, 1, 7, 7, 7, 7, 0, 9, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7,\n",
       "       7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 5, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 1, 7, 7, 7, 7, 7, 7, 7, 0, 9, 7,\n",
       "       7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 9, 7, 9, 7, 9, 9, 7, 9,\n",
       "       7, 7, 7, 7, 7, 7, 9, 9, 7, 7, 7, 9, 6, 7, 9, 7, 9, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 8, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 9, 7, 7,\n",
       "       7, 7, 9, 7, 7, 5, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 9, 7, 7, 7, 9, 9,\n",
       "       7, 9, 7, 7, 7, 7, 9, 7, 9, 1, 7, 9, 7, 7, 9, 7, 7, 7, 7, 7, 7, 9,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 0, 7, 7, 1, 9, 7, 7, 7, 7,\n",
       "       7, 7, 7, 6, 9, 7, 7, 7, 7, 7, 9, 9, 9, 7, 9, 1, 7, 7, 1, 1, 1, 1,\n",
       "       8, 3, 7, 7, 7, 7, 3, 3, 3, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 9,\n",
       "       7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 0, 9, 9, 7, 7, 9, 9, 7,\n",
       "       7, 7, 7, 9, 7, 9, 7, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 5, 7, 7, 7, 7, 7, 9, 7, 7, 7, 2, 7, 1, 7, 7, 7, 5, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 7, 8, 1, 7, 7, 7, 0, 7, 7, 9, 2, 7,\n",
       "       9, 9, 7, 9, 9, 7, 7, 9, 9, 7, 9, 7, 7, 9, 2, 2, 2, 2, 9, 7, 8, 9,\n",
       "       7, 8, 2, 7, 7, 0, 7, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 5, 9, 1, 1, 5, 9, 7, 9, 7, 1, 7,\n",
       "       7, 7, 7, 7, 7, 9, 7, 7, 9, 7, 9, 9, 7, 7, 7, 7, 1, 7, 7, 7, 9, 3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import os\n",
    "import logging\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmodyfikowana klasa od Wojtka i Staszka\n",
    "class BertBaseMultilingualEmbeddingVectorizer:\n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\", cuda=True):\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.tokens_tensor = None\n",
    "        self.segments_tensor = None\n",
    "        \n",
    "        self.encoded_layers_ = None\n",
    "        self.token_embeddings_ = None\n",
    "        \n",
    "        self.length=None\n",
    "        self.output_emb=None\n",
    "    \n",
    "    def _tokenize_text(self, text):\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        \n",
    "        self.tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)\n",
    "        self.segments_tensor = torch.tensor([segments_ids]).to(self.device)\n",
    "        \n",
    "    def _evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.model(self.tokens_tensor, self.segments_tensor)\n",
    "        #print(encoded_layers)\n",
    "        self.encoded_layers_ = encoded_layers\n",
    "    \n",
    "    def _generate_token_embeddings(self, batch_i=0):\n",
    "        \"\"\"\n",
    "        Convert the hidden state embeddings into single token vectors\n",
    "        Holds the list of 12 layer embeddings for each token\n",
    "        Will have the shape: [# tokens, # layers, # features]\n",
    "        \"\"\"\n",
    "        token_embeddings = [] \n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(self.encoded_layers_[-1][batch_i])):\n",
    "            # Holds 12 layers of hidden states for each token \n",
    "            hidden_layers = [] \n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(self.encoded_layers_)):\n",
    "            # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = self.encoded_layers_[layer_i][batch_i][token_i]\n",
    "                hidden_layers.append(vec)\n",
    "            token_embeddings.append(hidden_layers)\n",
    "        self.token_embeddings_ = token_embeddings\n",
    "    \n",
    "    def _compress_embeddings (self, counter):\n",
    "        how=\"mean_last_layer\" \n",
    "        if how == \"mean_last_layer\":\n",
    "            output_emb[counter,:]=np.array(torch.mean(self.encoded_layers_[-1], 1).squeeze()).reshape((1,-1))\n",
    "        elif how == \"mean_sum_last_4_layers\": \n",
    "            output_emb[counter,:]= np.array(torch.mean(torch.sum(torch.stack(self.encoded_layers_[-4:]), 0), 1).squeeze()).reshape((1,-1))\n",
    "\n",
    "    def transform(self, sentence):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence=[sentence]\n",
    "        self.length=len(sentence)\n",
    "        self.output_emb=np.zeros((self.length, 768))\n",
    "        counter=0\n",
    "        for i in sentence:\n",
    "            self._tokenize_text(i)\n",
    "            self._evaluate()\n",
    "            self._generate_token_embeddings()\n",
    "            self._compress_embeddings(counter)\n",
    "            counter=counter+1\n",
    "        return output_emb\n",
    "    \n",
    "    def fit_transform(self, sentence):\n",
    "        if isinstance(sentence, str): #obsługuje przypadek gdy robię predict dla jednego zdania a nie jak w lime dla listy\n",
    "            sentence=[sentence]\n",
    "        self.length=len(sentence)\n",
    "        self.output_emb=np.zeros((self.length, 768))\n",
    "        counter=0\n",
    "        for i in sentence:\n",
    "            self._tokenize_text(i)\n",
    "            self._evaluate()\n",
    "            self._generate_token_embeddings()\n",
    "            self._compress_embeddings(counter)\n",
    "            counter=counter+1\n",
    "        return output_emb\n",
    "            \n",
    "# transform i fit_transform są takie same ale są wymagane dwie oddzielne klasy przez make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "bert=BertBaseMultilingualEmbeddingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "c = make_pipeline(bert, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politycy']\n"
     ]
    }
   ],
   "source": [
    "#do szybkiego sprawdzania poprawności pipeline'u\n",
    "print(le.inverse_transform([np.argmax(c.predict_proba('Ale nie można w tym celu łamać prawa, bo za chwilę w podobnej sytuacji może się znaleźć nie tylko morderca-pedofil - powiedział.'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=le.inverse_transform(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\"bycie pierwszym na mecie zawodów to nagroda za ciężki trening\",c.predict_proba, num_features=10, top_labels=2)\n",
    "print(exp.available_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    <div style=\"background-color: #fdd; padding: 0.5em;\">\n",
       "        Error: estimator <tensorflow.python.keras.engine.sequential.Sequential object at 0x000001E3A7D04B88> is not supported\n",
       "    </div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# u nas nie działa\n",
    "import eli5\n",
    "eli5.show_weights(model, vec=bert, top=10,\n",
    "                  target_names=y_valid.unique())\n",
    "eli5.show_prediction(clf, twenty_test.data[0], vec=vec,\n",
    "                     target_names=twenty_test.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning models (random forest/ xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r'C:\\Users\\Paulina\\Downloads\\modele\\modele\\bert_xg_onet.pickle.dat', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1.0, gamma=1,\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=nan, n_estimators=300, n_jobs=-1,\n",
       "              nthread=None, objective='multi:softprob', random_state=12,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'duchowni', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy', 'politycy',\n",
       "       'politycy', 'politycy', 'politycy', 'politycy'], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
